{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfdc509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.responses import StreamingResponse\n",
    "from PIL import Image\n",
    "import torch\n",
    "import network\n",
    "from torchvision import transforms as T\n",
    "from datasets import VOCSegmentation, Cityscapes\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# --- Model Setup (adjust as needed) ---\n",
    "MODEL_NAME = 'deeplabv3plus_mobilenet'\n",
    "NUM_CLASSES = 19  # or 19 for cityscapes (was 21)\n",
    "DECODE_FN = VOCSegmentation.decode_target  # or Cityscapes.decode_target\n",
    "CKPT_PATH = './best_deeplabv3plus_mobilenet_cityscapes_os16.pth'#'path/to/your/checkpoint.pth'  # update this\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = network.modeling.__dict__[MODEL_NAME](num_classes=NUM_CLASSES, output_stride=16)\n",
    "checkpoint = torch.load(CKPT_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def overlay_mask_on_image(image: Image.Image, mask: Image.Image, alpha=0.5) -> Image.Image:\n",
    "    mask = mask.convert(\"RGBA\")\n",
    "    image = image.convert(\"RGBA\")\n",
    "    blended = Image.blend(image, mask, alpha)\n",
    "    return blended\n",
    "\n",
    "@app.post(\"/segment/\")\n",
    "async def segment_image(file: UploadFile = File(...)):\n",
    "    contents = await file.read()\n",
    "    img = Image.open(io.BytesIO(contents)).convert('RGB')\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(input_tensor).max(1)[1].cpu().numpy()[0]\n",
    "        color_mask = DECODE_FN(pred).astype('uint8')\n",
    "        mask_img = Image.fromarray(color_mask).resize(img.size)\n",
    "\n",
    "    overlayed = overlay_mask_on_image(img, mask_img, alpha=0.5)\n",
    "    buf = io.BytesIO()\n",
    "    overlayed.save(buf, format='PNG')\n",
    "    buf.seek(0)\n",
    "    return StreamingResponse(buf, media_type=\"image/png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6ad2e",
   "metadata": {},
   "source": [
    "How to use the provided code:\n",
    "1. Update CKPT_PATH and NUM_CLASSES as needed.\n",
    "2. Install FastAPI and Uvicorn:\n",
    "```bash\n",
    "pip install fastapi uvicorn\n",
    "```\n",
    "3. Start the server:\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "4. POST an image to /segment/ and receive the overlayed image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef48849",
   "metadata": {},
   "source": [
    "### Test client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ee2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Update this path to the image you want to test\n",
    "image_path = '../data/leftimg8bit/train/cologne/cologne_000129_000019_leftImg8bit.png' #\"test_image.jpg\"\n",
    "# The FastAPI server URL\n",
    "url = \"http://127.0.0.1:8000/segment/\"\n",
    "\n",
    "with open(image_path, \"rb\") as f:\n",
    "    files = {\"file\": (image_path, f, \"image/jpeg\")}\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"overlayed_result.png\", \"wb\") as out:\n",
    "        out.write(response.content)\n",
    "    print(\"Overlayed image saved as overlayed_result.png\")\n",
    "else:\n",
    "    print(\"Request failed:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87329199",
   "metadata": {},
   "source": [
    "##### How to use the test client:\n",
    "1. Place an image named test_image.jpg in your project folder (or update the path).\n",
    "2. Run your FastAPI app.\n",
    "3. Run this script:\n",
    "```python\n",
    "python test_client.py   \n",
    "```\n",
    "4. The overlayed result will be saved as overlayed_result.png.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf6808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-multipart\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: python-multipart\n",
      "Successfully installed python-multipart-0.0.20\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-multipart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff02aff",
   "metadata": {},
   "source": [
    "## Copilot code from browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import StreamingResponse\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load segmentation model\n",
    "model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"deeplabv3_resnet101\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Image transforms\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def segment_image(image: Image.Image) -> Image.Image:\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)[\"out\"][0]\n",
    "    mask = output.argmax(0).byte().cpu().numpy()\n",
    "\n",
    "    # Convert mask to RGBA\n",
    "    mask_rgba = Image.fromarray(np.uint8(mask * 10)).convert(\"L\")  # Multiply to improve contrast\n",
    "    color_mask = Image.new(\"RGBA\", image.size, (255, 0, 0, 100))  # Red with transparency\n",
    "\n",
    "    image_rgba = image.convert(\"RGBA\")\n",
    "    image_with_overlay = Image.composite(color_mask, image_rgba, mask_rgba)\n",
    "\n",
    "    return image_with_overlay\n",
    "\n",
    "@app.post(\"/segment/\")\n",
    "async def segment_upload(file: UploadFile = File(...)):\n",
    "    image = Image.open(io.BytesIO(await file.read())).convert(\"RGB\")\n",
    "    segmented = segment_image(image)\n",
    "\n",
    "    output_stream = io.BytesIO()\n",
    "    segmented.save(output_stream, format=\"PNG\")\n",
    "    output_stream.seek(0)\n",
    "\n",
    "    return StreamingResponse(output_stream, media_type=\"image/png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
